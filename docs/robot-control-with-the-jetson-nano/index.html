<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta http-equiv="Content-Security-Policy" content="default-src 'none'; script-src 'sha256-llOHzpSJ0BZ88zREq1LQZLs/qzXpSxIIL/XrAKNMBww='; style-src 'self'; img-src 'self'; connect-src 'self'; manifest-src 'self';">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link href="/assets/css/main.css" rel="stylesheet" />

        <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
        <link rel="manifest" href="/site.webmanifest">

        
        
        
            <meta name="description" content="Two years ago, I got to dive into a project at the intersection of machine learning and hardware - right before ChatGPT became a household name and when terms like overfitting puzzled me. Nonetheless, I was really excited about it, especially because it involved both software and hardware, a combination Im quite fond of.">
            <meta property="og:description" content="Two years ago, I got to dive into a project at the intersection of machine learning and hardware - right before ChatGPT became a household name and when terms like overfitting puzzled me. Nonetheless, I was really excited about it, especially because it involved both software and hardware, a combination Im quite fond of.">
            <meta name="description" content="Two years ago, I got to dive into a project at the intersection of machine learning and hardware - right before ChatGPT became a household name and when terms like overfitting puzzled me. Nonetheless, I was really excited about it, especially because it involved both software and hardware, a combination Im quite fond of."/>
        

        
        
            <meta property="og:image" content="/assets/images/robot-control-with-the-jetson-nano-social-image.jpg"/>
            <meta name="twitter:image" content="/assets/images/robot-control-with-the-jetson-nano-social-image.jpg"/>
        

        <title>
            
                Robot Control with the Jetson Nano | Nizar&#39;s Blog
            
        </title>

        <meta property="og:site_name" content="Nizar's Blog">
        <meta property="og:type" content="website">
        <script defer data-domain="nizar.se" src="https://plausible.io/js/script.js" integrity="sha256-llOHzpSJ0BZ88zREq1LQZLs/qzXpSxIIL/XrAKNMBww=" crossorigin="anonymous"></script>
    </head>
    <body>
        <div class="layout-wrapper">
            <header class="header">
    <div class="header__content">
        <h1 class="site-title">
            <a href="/">Nizar's Blog</a>
        </h1>
        <nav class="nav">
            <ul class="nav__list">
                <li class="nav-item">
                    <a href="/about">About</a>
                </li>
            </ul>
        </nav>
    </div>
</header>
            <main class="main">
                <article class="post">
    <header class="post__header">
        <h1>Robot Control with the Jetson Nano</h1>
        <div class="post__details">
            <time>02 Apr 2024</time>
            <span> | </span>
            <span>2 minutes</span>
        </div>
    </header>

    <div class="post__cover">
        <picture><source type="image/avif" srcset="/robot-control-with-the-jetson-nano/I96xVUqqDk-300.avif 300w, /robot-control-with-the-jetson-nano/I96xVUqqDk-600.avif 600w, /robot-control-with-the-jetson-nano/I96xVUqqDk-750.avif 750w, /robot-control-with-the-jetson-nano/I96xVUqqDk-900.avif 900w, /robot-control-with-the-jetson-nano/I96xVUqqDk-1200.avif 1200w, /robot-control-with-the-jetson-nano/I96xVUqqDk-1400.avif 1400w" sizes="Cover image"><source type="image/jpeg" srcset="/robot-control-with-the-jetson-nano/I96xVUqqDk-300.jpeg 300w, /robot-control-with-the-jetson-nano/I96xVUqqDk-600.jpeg 600w, /robot-control-with-the-jetson-nano/I96xVUqqDk-750.jpeg 750w, /robot-control-with-the-jetson-nano/I96xVUqqDk-900.jpeg 900w, /robot-control-with-the-jetson-nano/I96xVUqqDk-1200.jpeg 1200w, /robot-control-with-the-jetson-nano/I96xVUqqDk-1400.jpeg 1400w" sizes="Cover image"><img alt="Cover image" loading="lazy" decoding="async" src="/robot-control-with-the-jetson-nano/I96xVUqqDk-300.jpeg" width="1400" height="933"></picture>
        <p class="post__cover-caption">Side view of the assembled robot built for this project</p>
    </div>

    <main class="post__content">
        <p>Two years ago, I got to dive into a project at the intersection of machine learning and hardware - right before
ChatGPT became a household name and when terms like <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a> puzzled me.
Nonetheless, I was really excited about it, especially because it involved both software and hardware, a combination
I'm quite fond of.</p>
<p>The project was part of the <a href="https://developer.nvidia.com/embedded/learn/jetson-ai-certification-programs">Nvidia Jetson AI Certification program</a>,
which challenges participants to demonstrate their machine learning skills using the
<a href="https://developer.nvidia.com/embedded/jetson-nano-developer-kit">Jetson Nano Developer kit</a>.
Essentially, the kit is an embedded computer designed specifically to accelerate machine learning applications.
Though not required, I also had access to a <a href="https://www.waveshare.com/jetank-ai-kit.htm">robot</a>, which I was eager to
use.</p>
<p>I set a goal to train a model that would enable the robot's arm to be controlled via hand gestures.
The robot was equipped with a small camera on its front, perfectly positioned for this task.</p>
<h2>Image Classification</h2>
<p>I began with image classification, where the model is trained to identify and categorize images into specific classes.
My goal was to recognize four hand gestures:</p>
<ul>
<li>Thumbs up</li>
<li>Thumbs down</li>
<li>Fingers spread</li>
<li>Fingers together</li>
</ul>
<p><img src="/assets/images/animated/classification-feed-demo.gif" alt="Image classification demo of hand grip and gestures"></p>
<p>This allowed me to command the robot's arm and grip movement using these gestures. However, it was somewhat hit-or-miss.
The approach was basic, capable of only recognizing one gesture at a time, which limited the ability to control the
arm's elevation and grip simultaneously.</p>
<p>Moreover, the control was binary; It lacked the nuance to specify degrees of movement or grip tightness, posing a
challenge for precise control.</p>
<p><img src="/assets/images/animated/classification-grip-demo.gif" alt="Demo of the robot control using image classification"></p>
<h2>Image Regression</h2>
<p>Moving forward, I explored image regression. Unlike classification, this method involved training the model to pinpoint
my hand's features in images, providing coordinates for my fingertips. The challenge then became translating these
coordinates into accurate robot movements.</p>
<p><img src="/assets/images/animated/regression-feed-index.gif" alt="Image regression demo of hand grip"></p>
<p>A significant hurdle was accounting for the camera's depth perception, essential for distinguishing between fingertips
being spread apart versus the hand moving closer to the camera.</p>
<p>I experimented with measuring distances between various hand landmarks as proxies for depth perception, such as the
distance from the thumb's top and the wrist's bottom corner, or between the tips of the index and pinky fingers.
This approach, however, introduced additional complexity and new challenges, such as adjusting for the hand tilting and
panning.</p>
<p><picture><source type="image/avif" srcset="/robot-control-with-the-jetson-nano/nVGq2UthdK-300.avif 300w, /robot-control-with-the-jetson-nano/nVGq2UthdK-600.avif 600w, /robot-control-with-the-jetson-nano/nVGq2UthdK-750.avif 750w, /robot-control-with-the-jetson-nano/nVGq2UthdK-900.avif 900w" sizes="100vw"><source type="image/jpeg" srcset="/robot-control-with-the-jetson-nano/nVGq2UthdK-300.jpeg 300w, /robot-control-with-the-jetson-nano/nVGq2UthdK-600.jpeg 600w, /robot-control-with-the-jetson-nano/nVGq2UthdK-750.jpeg 750w, /robot-control-with-the-jetson-nano/nVGq2UthdK-900.jpeg 900w" sizes="100vw"><img alt="animation showing image regression training of index finger" loading="lazy" decoding="async" src="/robot-control-with-the-jetson-nano/nVGq2UthdK-300.jpeg" width="900" height="217"></picture></p>
<p>Ultimately, the model's error margins and the camera's limitations led me to simplify my approach. I chose to track only
the thumb and index finger while maintaining a constant distance from the camera. This simpler model achieved more
predictable results and easier control.</p>
<p><img src="/assets/images/animated/regression-day-7.gif" alt="Demo of the robot control using image regression"></p>
<p>While the project may seem straightforward, it was incredibly insightful, making the seemingly complex field of machine
learning feel much more attainable.</p>
<p>Every so often, I reflect on this project and wonder if a simpler solution exists for accurately considering distance.
An intriguing possibility that comes to mind is the use of two cameras, mirroring the binocular vision of humans, to
facilitate a more straightforward approach to depth perception. This idea excites me, and I'm keen to explore it
further, possibly revisiting the project with a new perspective in the future.</p>
<p>You can find the source code and assembly instruction on my GitHub <a href="https://github.com/nizos/jetarm">repo</a>.
If you are curious about machine learning or robotics, I encourage you to give it a look - who knows what you might
discover?</p>

    </main>

    <aside class="post__aside">
        <div class="post__tags">
            
            
                
                <a href="/tags/ai/">#ai</a>
            
                
                <a href="/tags/cv/">#cv</a>
            
                
                <a href="/tags/robotics/">#robotics</a>
            
                
                <a href="/tags/ml/">#ml</a>
            
        </div>
    </aside>
</article>

            </main>

            <footer class="footer">
    <div class="footer__content">
        <p class="footer__attribution">Powered by <a href="https://www.11ty.dev">11ty</a>, based on <a href="https://github.com/yinkakun/eleventy-duo">Eleventy Duo</a></p>
    </div>
</footer>
        </div>
    </body>
</html>